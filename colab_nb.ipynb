{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from typing import Optional, Any, Callable\n",
    "import numpy as np\n",
    "from skimage.draw import polygon\n",
    "from skimage.filters import gaussian\n",
    "from skimage.feature import peak_local_max\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import os, glob, copy, random, pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x = self.bn1(self.conv1(x_in))\n",
    "        x = F.relu(x)\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return x + x_in\n",
    "\n",
    "class GRConvNet4(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels=4, output_channels=1, channel_size=32, dropout=False, prob=0.0, clip=False):\n",
    "        super(GRConvNet4, self).__init__()\n",
    "        self.clip = clip\n",
    "        self.conv1 = nn.Conv2d(input_channels, channel_size, kernel_size=9, stride=1, padding=4)\n",
    "        self.bn1 = nn.BatchNorm2d(channel_size)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channel_size, channel_size // 2, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channel_size // 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channel_size // 2, channel_size // 4, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(channel_size // 4)\n",
    "\n",
    "        self.res1 = ResidualBlock(channel_size // 4, channel_size // 4)\n",
    "        self.res2 = ResidualBlock(channel_size // 4, channel_size // 4)\n",
    "        self.res3 = ResidualBlock(channel_size // 4, channel_size // 4)\n",
    "        self.res4 = ResidualBlock(channel_size // 4, channel_size // 4)\n",
    "        self.res5 = ResidualBlock(channel_size // 4, channel_size // 4)\n",
    "\n",
    "        self.conv4 = nn.ConvTranspose2d(channel_size // 4, channel_size // 2, kernel_size=4, stride=2, padding=1,\n",
    "                                        output_padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(channel_size // 2)\n",
    "\n",
    "        self.conv5 = nn.ConvTranspose2d(channel_size // 2, channel_size, kernel_size=4, stride=2, padding=2,\n",
    "                                        output_padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(channel_size)\n",
    "\n",
    "        self.conv6 = nn.ConvTranspose2d(channel_size, channel_size, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "        self.grasp_outputs = nn.Conv2d(in_channels=channel_size, out_channels=output_channels*4, kernel_size=2)\n",
    "        self.confidence_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.dropout_conf = nn.Dropout(p=prob)\n",
    "        self.dropout_grasp = nn.Dropout(p=prob)\n",
    "\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x = F.relu(self.bn1(self.conv1(x_in)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(x)\n",
    "        x = self.res5(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        if self.dropout:\n",
    "            grasp_output = self.grasp_outputs(self.dropout_grasp(x))\n",
    "            confidence_output = self.confidence_output(self.dropout_conf(x))\n",
    "        else:\n",
    "            grasp_output = self.grasp_outputs(x)\n",
    "            confidence_output = self.confidence_output(x)\n",
    "\n",
    "        confidence_output = F.sigmoid(confidence_output)      \n",
    "        grasp_output = F.tanh(grasp_output)  \n",
    "\n",
    "        output = torch.cat([confidence_output, grasp_output], dim=1)\n",
    "        if self.clip:\n",
    "            output = output.clip(-1, 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parent class\n",
    "class JacquardGraspCLSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Parent class for all dataset objects dealing with the Jacquard Dataset. Handles file handling and combining \n",
    "    file paths for every instance of training data. Also handles train and test splitting.\n",
    "    \n",
    "    Assumes that all jacquard dataset instances have been cleaned using the methods in dataset/preprocess.py\n",
    "\n",
    "    Child classes must implement methods:\n",
    "        - get_instance_from_dataset - Gets a single instance from training data given file paths. Handle any augmentations here.\n",
    "        - get_instance_from_cache - Gets a single instance of training data from the cached files (only use if heavy preprocessing)\n",
    "        - visualize_instance - visualizes a single instance of training data\n",
    "        - cache_dataset - caches the entire dataset (only use if heavy preprocessing)\n",
    "\n",
    "    Cache assumes the following directory structure:\n",
    "    cache_location\n",
    "        |__ class_1\n",
    "            |__ training_data_1.npz\n",
    "            |__ training_data_2.npz\n",
    "            ...\n",
    "        |__ class_2\n",
    "        ...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            image_size: int, \n",
    "            dataset_path: Optional[str] = None, \n",
    "            cache_path: Optional[str] = None,\n",
    "            random_augment: bool = True\n",
    "        ) -> None:\n",
    "        assert dataset_path is not None or cache_path is not None, \"One of dataset_path or cache_path must be given\"\n",
    "        assert dataset_path is None or cache_path is None, \"One of dataset_path or cache_path much be left empty\"\n",
    "        \n",
    "        self.dataset_path = dataset_path\n",
    "        self.random_augment = random_augment\n",
    "        self.cache_path = cache_path\n",
    "        if self.dataset_path is not None:\n",
    "            self.class_to_idx, self.idx_to_class = self.get_class_map(self.dataset_path)    \n",
    "        else:\n",
    "            self.class_to_idx, self.idx_to_class = self.get_class_map(self.cache_path)\n",
    "        self.image_size = image_size\n",
    "        self.individual_file_paths = self.get_all_file_paths(self.dataset_path)\n",
    "\n",
    "    def extract_test_dataset(self, test_split_ratio: float):\n",
    "        \"\"\"\n",
    "        Extracts test_split_ratio * len(self) number of data points into a test dataset and returns it.\n",
    "        Also turns off random_augment for the test dataset.\n",
    "        \"\"\"\n",
    "        random.seed(0)\n",
    "        test_dataset = copy.copy(self)\n",
    "\n",
    "        self.train_idxs = random.sample(list(range(self.__len__())), k = int((1 - test_split_ratio) * self.__len__()), )\n",
    "        self.test_idxs = [i for i in range(self.__len__()) if i not in self.train_idxs]\n",
    "\n",
    "        train_fps = [self.individual_file_paths[i] for i in self.train_idxs]\n",
    "        test_fps = [self.individual_file_paths[i] for i in self.test_idxs]\n",
    "\n",
    "        self.individual_file_paths = train_fps\n",
    "        test_dataset.individual_file_paths = test_fps\n",
    "        test_dataset.random_augment = False\n",
    "        return test_dataset\n",
    "\n",
    "    def get_instance_from_dataset(self, file_paths: list[str], random_augment: bool = True) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        To be implemented by a child class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_instance_from_cache(self, file_path: list[str], random_augment: bool = True) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        To be implemented by a child class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def visualize_instance(self, file_paths: list[str]) -> None:\n",
    "        \"\"\"\n",
    "        To be implemented by a child class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def cache_dataset(self, cache_location: str):\n",
    "        \"\"\"\n",
    "        To be implemented by a child class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_paths = self.individual_file_paths[idx]\n",
    "        if self.dataset_path is not None:\n",
    "            return_values = self.get_instance_from_dataset(file_paths, random_augment=self.random_augment)\n",
    "        else:\n",
    "            return_values = self.get_instance_from_cache(file_paths, random_augment=self.random_augment)\n",
    "        return return_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.individual_file_paths)\n",
    "    \n",
    "    def visualize(self, idx) -> None:\n",
    "        self.visualize_instance(self.individual_file_paths[idx])\n",
    "    \n",
    "    def get_all_file_paths(self, dataset_path: str) -> list[list[str]]:\n",
    "        \"\"\"\n",
    "        Returns a list of lists \\n\n",
    "        Each list contains the file paths for the files in the following order:\n",
    "            - rgb image file paths\n",
    "            - perfect depth file paths\n",
    "            - stereo depth file paths\n",
    "            - grasp file paths\n",
    "            - mask file paths\n",
    "            - class of object (not a path)\n",
    "\n",
    "        If loading from cache, outputs a list containing a single file path\n",
    "        \"\"\"\n",
    "        if self.dataset_path is not None:\n",
    "            rgb_paths = glob.glob(os.path.join(dataset_path, \"*/*\", \"*RGB.png\"))\n",
    "            output = []\n",
    "            for rgb_path in rgb_paths:\n",
    "                assert os.path.isfile(rgb_path)\n",
    "                instance_data = [rgb_path]\n",
    "\n",
    "                perfect_depth_path = rgb_path.replace(\"RGB.png\", \"perfect_depth.tiff\")\n",
    "                assert os.path.isfile(perfect_depth_path)\n",
    "                instance_data.append(perfect_depth_path)\n",
    "\n",
    "                stereo_depth_path = rgb_path.replace(\"RGB.png\", \"stereo_depth.tiff\")\n",
    "                assert os.path.isfile(stereo_depth_path)\n",
    "                instance_data.append(stereo_depth_path)\n",
    "\n",
    "                grasps_path = rgb_path.replace(\"RGB.png\", \"grasps.txt\")\n",
    "                assert os.path.isfile(grasps_path)\n",
    "                instance_data.append(grasps_path)\n",
    "\n",
    "                mask_path = rgb_path.replace(\"RGB.png\", \"mask.png\")\n",
    "                assert os.path.isfile(mask_path)\n",
    "                instance_data.append(mask_path)\n",
    "\n",
    "                instance_data.append(rgb_path.split(\"/\")[-3])\n",
    "                output.append(instance_data)\n",
    "            return output\n",
    "        else:\n",
    "            return glob.glob(os.path.join(self.cache_path, \"*/*\"))\n",
    "            \n",
    "    def get_class_map(self, dataset_path: str) -> tuple[dict[str, int], dict[int, str]]:\n",
    "        all_classes = self.listdir(dataset_path)\n",
    "        class_to_idx = {c:i for i, c in enumerate(sorted(all_classes))}\n",
    "        idx_to_class = {v:k for k, v in class_to_idx.items()}\n",
    "        return class_to_idx, idx_to_class\n",
    "\n",
    "    def listdir(self, folder_path: str) -> list[str]:\n",
    "        files = os.listdir(folder_path)\n",
    "        fn = [i for i in files if \".DS_Store\" in i]\n",
    "        if len(fn) > 0:\n",
    "            files.remove(fn[0])\n",
    "        return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Child class\n",
    "class MapBasedJacquardDataset(JacquardGraspCLSDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        image_size: int, \n",
    "        precision: torch.dtype,\n",
    "        dataset_path: Optional[str] = None, \n",
    "        cache_path: Optional[str] = None,\n",
    "        random_augment: bool = True,\n",
    "        width_scale_factor: int = 1\n",
    "    ) -> None:\n",
    "        super().__init__(image_size, dataset_path, cache_path, random_augment)\n",
    "        self.width_scale_factor = width_scale_factor\n",
    "        self.precision = precision\n",
    "\n",
    "    ###### Image and depth map loading functions\n",
    "    def load_rgbd_image(self, rgb_image_path: str, depth_image_path: str) -> torch.Tensor:\n",
    "        T = transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        rgb_image = T(Image.open(rgb_image_path))[:3]\n",
    "        depth_image = T(Image.open(depth_image_path))\n",
    "        return torch.cat([rgb_image, depth_image], dim=0)\n",
    "    \n",
    "    def preprocess_rbgd_image(self, rgbd_image: torch.Tensor) -> torch.Tensor:\n",
    "        ## Add preprocessing steps here (normalization, etc..)\n",
    "        return rgbd_image\n",
    "    \n",
    "\n",
    "    ###### Classification label loading functions\n",
    "    def load_mask_image(self, mask_path: str) -> torch.Tensor:\n",
    "        T = transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        return T(Image.open(mask_path))\n",
    "    \n",
    "    def load_classification_labels(self, mask_image: torch.Tensor, class_label: str):\n",
    "        \"\"\"\n",
    "        The output of this function has two parts;\n",
    "            - output[0] is the object/background mask. This map contains a 0 for all background pixels\n",
    "              and a 1 for all object pixels. A sigmoid non-linearity is applied on the first channel of the model\n",
    "              output to reflect this.\n",
    "            - output[1:] are the class maps. This map contains a 0 value for all background pixels. When the class_map index\n",
    "              matches the target class index, the object pixels are valued at 1 and when it does not, the object pixels\n",
    "              are valued at -1. A tanh non-linearity is applied to the remaining channels of the model output\n",
    "              to reflect this.\n",
    "\n",
    "        Use self.visualize(idx) to visualize instances of cls_maps.\n",
    "        \"\"\"\n",
    "        mask_image = mask_image.round()\n",
    "        class_idx = self.class_to_idx[class_label]\n",
    "\n",
    "        # Initially the cls maps are entirely -1 valued.\n",
    "        class_maps = torch.ones(len(self.class_to_idx), mask_image.shape[1], mask_image.shape[2]) * -1\n",
    "        # changing the cls map of the correct class to be +1 valued.\n",
    "        class_maps[class_idx] *= -1\n",
    "\n",
    "        # making all non-object, background pixels 0\n",
    "        class_maps = class_maps * mask_image.repeat(len(self.class_to_idx), 1, 1)\n",
    "        return torch.cat([mask_image, class_maps], dim=0)\n",
    "\n",
    "\n",
    "    ###### Grasp label loading functions\n",
    "    def load_grasp_file(self, grasp_path: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns in the order [y, x, angle, length, width]\n",
    "        \"\"\"\n",
    "        grasps = []\n",
    "        with open(grasp_path, \"r\") as f:\n",
    "            for l in f:\n",
    "                x, y, theta, w, h = [float(v) for v in l[:-1].split(';')]\n",
    "                grasps.append([x, y, -theta / 180.0 * np.pi, w * self.width_scale_factor, h])\n",
    "        grasps = np.array(grasps)\n",
    "\n",
    "        # rescaling values based on image size\n",
    "        grasps[:, :2] *= (self.image_size / 1024)\n",
    "        grasps[:, 3:] *= (self.image_size / 1024)\n",
    "        return grasps\n",
    "    \n",
    "    def compute_grasp_rectangle(self, grasp_arr: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Converts the jacquard dataset grasps into grasp rectangles \n",
    "        (returns coordinates of 4 corners of each rectangle)\n",
    "        \"\"\"\n",
    "        x, y, angle, length, width = grasp_arr.transpose()\n",
    "        xo = np.cos(angle)\n",
    "        yo = np.sin(angle)\n",
    "\n",
    "        ya = y + length / 2 * yo\n",
    "        xa = x - length / 2 * xo\n",
    "        yb = y - length / 2 * yo\n",
    "        xb = x + length / 2 * xo\n",
    "\n",
    "        y1, x1 = ya - width / 2 * xo, xa - width / 2 * yo\n",
    "        y2, x2 = yb - width / 2 * xo, xb - width / 2 * yo\n",
    "        y3, x3 = yb + width / 2 * xo, xb + width / 2 * yo\n",
    "        y4, x4 = ya + width / 2 * xo, xa + width / 2 * yo\n",
    "\n",
    "        # p1 = np.stack([y1, x1], 1)\n",
    "        # p2 = np.stack([y2, x2], 1)\n",
    "        # p3 = np.stack([y3, x3], 1)\n",
    "        # p4 = np.stack([y4, x4], 1)\n",
    "\n",
    "        p1 = np.stack([x1, y1], 1)\n",
    "        p2 = np.stack([x2, y2], 1)\n",
    "        p3 = np.stack([x3, y3], 1)\n",
    "        p4 = np.stack([x4, y4], 1)\n",
    "\n",
    "        output_arr = np.stack([p1, p2, p3, p4], 1)\n",
    "        return output_arr\n",
    "    \n",
    "    def compute_grasp_map(\n",
    "            self,\n",
    "            grasp_arr: np.ndarray, \n",
    "            grasp_rect_array: np.ndarray\n",
    "        ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns a tuple containing torch tensors of shape (image_size, image_size) in\n",
    "        the order (confidence_map, cos, sin, width)\n",
    "        \"\"\"\n",
    "        conf_out = np.zeros((self.image_size, self.image_size))\n",
    "        angle_out = np.zeros((self.image_size, self.image_size))\n",
    "        width_out = np.zeros((self.image_size, self.image_size))\n",
    "        length_out = np.zeros((self.image_size, self.image_size))\n",
    "\n",
    "        for rect, (_, _, angle, width, length) in zip(grasp_rect_array, grasp_arr):\n",
    "            cc, rr = polygon(rect[:, 0], rect[:, 1], (self.image_size, self.image_size))\n",
    "            conf_out[rr, cc] = 1.0\n",
    "            angle_out[rr, cc] = angle\n",
    "            width_out[rr, cc] = width\n",
    "            length_out[rr, cc] = length\n",
    "        width_out /= self.image_size\n",
    "        length_out /= self.image_size\n",
    "\n",
    "        out_tensor = torch.cat([\n",
    "            torch.from_numpy(conf_out).unsqueeze(0),\n",
    "            torch.from_numpy(np.cos(angle_out)).unsqueeze(0),\n",
    "            torch.from_numpy(np.sin(angle_out)).unsqueeze(0),\n",
    "            torch.from_numpy(width_out).unsqueeze(0),\n",
    "            torch.from_numpy(length_out).unsqueeze(0)], dim=0)\n",
    "        return out_tensor\n",
    "    \n",
    "    def rotate_augment(\n",
    "            self,\n",
    "            rotation_angle: int, \n",
    "            rgbd_image: torch.Tensor, \n",
    "            grasp_map: torch.Tensor,\n",
    "            grasp_rect: torch.Tensor,\n",
    "            cls_map: torch.Tensor,\n",
    "            rotate: Optional[bool] = None\n",
    "        ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        if self.random_augment or rotate:\n",
    "            if rotation_angle == 270:\n",
    "                rgbd_image = rgbd_image.transpose(-1, -2)\n",
    "                grasp_map = grasp_map.transpose(-1, -2)\n",
    "                grasp_rect = grasp_rect.flip(-1)\n",
    "                cls_map = cls_map.transpose(-1, -2)\n",
    "            elif rotation_angle == 180:\n",
    "                rgbd_image = rgbd_image.flip(-2)\n",
    "                grasp_map = grasp_map.flip(-2)\n",
    "                grasp_rect[:, :, 1] = self.image_size - grasp_rect[:, :, 1]\n",
    "                cls_map = cls_map.flip(-2)\n",
    "            elif rotation_angle == 90:\n",
    "                rgbd_image = rgbd_image.flip(-2).transpose(-1, -2)\n",
    "                grasp_map = grasp_map.flip(-2).transpose(-1, -2)\n",
    "                grasp_rect = grasp_rect.flip(-1)\n",
    "                grasp_rect[:, :, 0] = self.image_size - grasp_rect[:, :, 0]\n",
    "                cls_map = cls_map.flip(-2).transpose(-1, -2)\n",
    "            elif rotation_angle == 0:\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception(\"Invalid rotation angle\")\n",
    "        return rgbd_image, grasp_map, grasp_rect, cls_map\n",
    "    \n",
    "    def get_random_rotation_angle(self):\n",
    "        return random.choice([0, 90, 180, 270])\n",
    "    \n",
    "    def cast_to_fp_precision(self, rgbd, grasp_map, grasp_rect, cls_map):\n",
    "        rgbd = rgbd.to(self.precision)\n",
    "        grasp_map = grasp_map.to(self.precision)\n",
    "        grasp_rect = grasp_rect.to(self.precision)\n",
    "        cls_map = cls_map.to(self.precision)\n",
    "        return rgbd, grasp_map, grasp_rect, cls_map\n",
    "\n",
    "    def get_instance_from_dataset(self, file_paths: list[str], random_augment: bool = True) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        rgb_fp, perfect_depth_fp, stereo_depth_fp, grasp_fp, mask_fp, class_label = file_paths\n",
    "\n",
    "        rgbd_image = self.load_rgbd_image(rgb_fp, perfect_depth_fp)\n",
    "        rgbd_image = self.preprocess_rbgd_image(rgbd_image)\n",
    "\n",
    "        mask_image = self.load_mask_image(mask_fp)\n",
    "        cls_labels = self.load_classification_labels(mask_image, class_label)\n",
    "\n",
    "        grasp_arr = self.load_grasp_file(grasp_fp)\n",
    "        grasp_rect_arr = self.compute_grasp_rectangle(grasp_arr)\n",
    "        grasp_map = self.compute_grasp_map(grasp_arr, grasp_rect_arr)\n",
    "        grasp_rect_arr = torch.from_numpy(grasp_rect_arr)\n",
    "\n",
    "        rgbd_image, grasp_map, grasp_rect, cls_labels = self.rotate_augment(self.get_random_rotation_angle(), rgbd_image, grasp_map, grasp_rect_arr, cls_labels)\n",
    "        rgbd_image, grasp_map, grasp_rect, cls_labels = self.cast_to_fp_precision(rgbd_image, grasp_map, grasp_rect, cls_labels)\n",
    "\n",
    "        return rgbd_image, grasp_map, grasp_rect, cls_labels\n",
    "    \n",
    "    def get_instance_from_cache(self, file_paths: str, random_augment: bool = True) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        rgbd, grasp_map, grasp_rect, cls_map = np.load(file_paths).values()\n",
    "        rgbd, grasp_map, grasp_rect, cls_map = torch.from_numpy(rgbd), torch.from_numpy(grasp_map), torch.from_numpy(grasp_rect), torch.from_numpy(cls_map)\n",
    "        rgbd, grasp_map, grasp_rect, cls_map = self.rotate_augment(self.get_random_rotation_angle(), rgbd, grasp_map, grasp_rect, cls_map)\n",
    "        rgbd, grasp_map, grasp_rect, cls_map = self.cast_to_fp_precision(rgbd, grasp_map, grasp_rect, cls_map)\n",
    "        return rgbd, grasp_map, grasp_rect, cls_map\n",
    "    \n",
    "    def visualize_instance(self, file_paths: list[str]) -> None:\n",
    "        if self.dataset_path is not None:\n",
    "            rgbd_image, (conf, cos, sin, width, length), grasp_rect,  cls_labels = self.get_instance_from_dataset(file_paths)\n",
    "        else:\n",
    "            rgbd_image, (conf, cos, sin, width, length), grasp_rect, cls_labels = self.get_instance_from_cache(file_paths)\n",
    "        \n",
    "        fig1, ax1 = plt.subplots(nrows=1, ncols=3, figsize=(20, 10))\n",
    "        ax1 = ax1.flatten()\n",
    "\n",
    "        ax1[0].imshow(rgbd_image[:-1].permute(1, 2, 0))\n",
    "        ax1[0].axis(False)\n",
    "        ax1[0].set_title(\"RGB Image\")\n",
    "\n",
    "        ax1[1].imshow(rgbd_image[-1])\n",
    "        ax1[1].axis(False)\n",
    "        ax1[1].set_title(\"Depth Map\")\n",
    "\n",
    "        ax1[2].imshow(rgbd_image[:-1].permute(1, 2, 0))\n",
    "        for rect in grasp_rect:\n",
    "            rect = Polygon(rect, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
    "            ax1[2].add_patch(rect)\n",
    "        ax1[2].axis(False)\n",
    "        ax1[2].set_title(\"Bounding boxes\")\n",
    "        plt.show()\n",
    "\n",
    "        fig2, ax2 = plt.subplots(nrows=1, ncols=5, figsize=(20, 6))\n",
    "        ax2 = ax2.flatten()\n",
    "        for ax, img, name in zip(ax2, (conf, cos, sin, width, length), (\"Confidence map\", \"cos\", \"sin\", \"width\", \"length\")):\n",
    "            im = ax.imshow(img)\n",
    "            ax.axis(False)\n",
    "            ax.set_title(name)\n",
    "        plt.show()\n",
    "\n",
    "        fig3, ax3 = plt.subplots(nrows=1, ncols=len(self.class_to_idx) + 1, figsize=(20, 6))\n",
    "        ax3 = ax3.flatten()\n",
    "        for ax, img, idx in zip(ax3, cls_labels, range(len(self.class_to_idx) + 1)):\n",
    "            im = ax.imshow(img, vmin=-1, vmax=1)\n",
    "            ax.axis(False)\n",
    "            if idx == 0:\n",
    "                ax.set_title(\"Background/Object\")\n",
    "            else:\n",
    "                ax.set_title(self.idx_to_class[idx - 1])\n",
    "        plt.colorbar(im)\n",
    "        plt.show()\n",
    "\n",
    "    def cache_dataset(self, cache_location: str):\n",
    "        os.mkdir(cache_location)\n",
    "        for class_name in self.class_to_idx.keys():\n",
    "            class_path = os.path.join(cache_location, class_name)\n",
    "            os.mkdir(class_path)\n",
    "\n",
    "        class_item_indices = {cn: 0 for cn in self.class_to_idx.keys()}\n",
    "        print(\"Creating cache...\")\n",
    "        loop = tqdm(range(len(self)))\n",
    "        self.random_augment = False\n",
    "        for i in loop:\n",
    "            rgbd, grasp_map, grasp_rect_arr, cls_map = self[i]\n",
    "            class_name = self.individual_file_paths[i][-1]\n",
    "            save_dir = os.path.join(cache_location, class_name, class_name + \"_\" + str(class_item_indices[class_name]))\n",
    "            class_item_indices[class_name] += 1\n",
    "\n",
    "            rgbd, grasp_map, cls_map = rgbd.numpy(), grasp_map.numpy(), cls_map.numpy()\n",
    "            np.savez_compressed(save_dir, rgbd=rgbd, grasp_map=grasp_map, grasp_rect=grasp_rect_arr, cls_map=cls_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "class DoubleLogLoss:\n",
    "\n",
    "    def __init__(self, mean_reduction: bool = True):\n",
    "        self.mean_reduction = mean_reduction\n",
    "\n",
    "    def check_range(self, y, yhat):\n",
    "        assert ((y < -1).sum() + (y > 1).sum()).item() == 0, \"Target outside valid range\"\n",
    "        assert ((yhat < -1).sum() + (yhat > 1).sum()).item() == 0, \"Predicted value outside valid range\"\n",
    "\n",
    "    def fp_error_recentre(self, yhat, tolerance = 1e-5):\n",
    "        max_filter_map = torch.zeros_like(yhat)\n",
    "        min_filter_map = torch.zeros_like(yhat)\n",
    "        max_filter_map[yhat > 1] += tolerance\n",
    "        min_filter_map[yhat < -1] += tolerance\n",
    "        yhat = yhat - max_filter_map\n",
    "        yhat = yhat + min_filter_map\n",
    "        return yhat\n",
    "\n",
    "    def __call__(self, yhat, y):\n",
    "        \"\"\"\n",
    "        yhat should be a tensor of predictions of shape [batch_size, num_classes], with each element being\n",
    "        between -1 and 1 (tanh activation after final layer).\n",
    "\n",
    "        y should be a tensor of target labels of shape [batch_size, num_classes].\n",
    "        IMPORTANT_NOTE : each element of y will be a tensor of shape [num_classes]. Each element of this\n",
    "                        SHOULD be either -1 or 1 when training for cls, with -1 (not 0) indicating that the object\n",
    "                        is not a particular class and 1 indicating that it is.\n",
    "        \"\"\"\n",
    "        self.check_range(y=y, yhat=yhat)\n",
    "        y_less_than_loss = - torch.log(1 + (1 / (1 + yhat + 1e-5))*(y - yhat))\n",
    "        yhat_less_than_loss = - torch.log(1 + (1 / (1 - yhat + 1e-5))*(yhat - y))\n",
    "        output = torch.where(y < yhat, y_less_than_loss, yhat_less_than_loss)\n",
    "        if self.mean_reduction:\n",
    "            output = output.mean()\n",
    "        return output\n",
    "    \n",
    "class DoubleLogMapLoss:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.double_log = DoubleLogLoss(mean_reduction=False)\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def __call__(self, predicted_map: torch.Tensor, target_map: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        predicted_map and target_map are of shape [batch_size, n_channels, img_size, img_size].\n",
    "        indexing with [:, 0, :, :] represents confidence maps for both predicted and target.\n",
    "        \"\"\"\n",
    "        # Reshape to [n_channels, batch_size, img_size, img_size]\n",
    "        predicted_map = predicted_map.permute(1, 0, 2, 3)\n",
    "        target_map = target_map.permute(1, 0, 2, 3)\n",
    "\n",
    "        # Computing a confidence map loss with the first channel\n",
    "        confidence_loss = self.bce(predicted_map[0], target_map[0])\n",
    "\n",
    "        # Computing the grasp/cls losses with remaining channels\n",
    "        grasp_cls_loss = self.double_log(predicted_map[1:], target_map[1:])\n",
    "\n",
    "        # Valid pixels for grasp_cls_loss are those where the target confidence map is not 0, \n",
    "        # since these are pixels which belong to the object and not the background.\n",
    "        valid_pixels = target_map[0] != 0\n",
    "        valid_pixels = valid_pixels.unsqueeze(0).repeat(predicted_map.shape[0] - 1, 1, 1, 1)\n",
    "        grasp_cls_loss = (grasp_cls_loss * valid_pixels).mean()\n",
    "        return confidence_loss + grasp_cls_loss * 2\n",
    "    \n",
    "class CrossEntropyMapLoss:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def __call__(self, predicted_map: torch.Tensor, target_map: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        predicted_map and target_map are of shape [batch_size, n_channels, img_size, img_size].\n",
    "        indexing with [:, 0, :, :] represents confidence maps for both predicted and target.\n",
    "        \"\"\"\n",
    "        # Reshape to [n_channels, batch_size, img_size, img_size]\n",
    "        predicted_map = predicted_map.permute(1, 0, 2, 3)\n",
    "        target_map = target_map.permute(1, 0, 2, 3)\n",
    "\n",
    "        confidence_predicted_map = predicted_map[0]\n",
    "        confidence_target_map = target_map[0]\n",
    "        grasp_or_cls_predictions = predicted_map[1:]\n",
    "        grasp_or_cls_targets = target_map[1:]\n",
    "\n",
    "        # Computing a confidence map loss with the first channel\n",
    "        confidence_loss = self.bce(confidence_predicted_map, confidence_target_map)\n",
    "\n",
    "        # Rescaling predicted and target outputs to be between 0 and 1 for bce loss\n",
    "        grasp_or_cls_predictions = (grasp_or_cls_predictions + 1) / 2\n",
    "        grasp_or_cls_targets = (grasp_or_cls_targets + 1) / 2\n",
    "\n",
    "        # Computing the grasp/cls losses with remaining channels\n",
    "        grasp_cls_loss = self.bce(grasp_or_cls_predictions, grasp_or_cls_targets)\n",
    "\n",
    "        # Valid pixels for grasp_cls_loss are those where the target confidence map is not 0, \n",
    "        # since these are pixels which belong to the object and not the background.\n",
    "        valid_pixels = target_map[0] != 0\n",
    "        valid_pixels = valid_pixels.unsqueeze(0).repeat(predicted_map.shape[0] - 1, 1, 1, 1)\n",
    "        grasp_cls_loss = (grasp_cls_loss * valid_pixels).mean()\n",
    "        return confidence_loss + grasp_cls_loss * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grasp utility functions\n",
    "def post_process_map_output(\n",
    "        q_img: torch.Tensor, \n",
    "        cos_img: torch.Tensor, \n",
    "        sin_img: torch.Tensor, \n",
    "        width_img: torch.Tensor,\n",
    "        length_img: torch.Tensor,\n",
    "    ) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    q_img = q_img.cpu().numpy().squeeze()\n",
    "    ang_img = (torch.atan2(sin_img, cos_img) / 2.0).cpu().numpy().squeeze()\n",
    "    width_img = width_img.cpu().numpy().squeeze()\n",
    "    width_img *= width_img.shape[-1]\n",
    "    length_img = length_img.cpu().numpy().squeeze()\n",
    "    length_img *= length_img.shape[-1]\n",
    "\n",
    "    q_img = gaussian(q_img, 2.0, preserve_range=True)\n",
    "    ang_img = gaussian(ang_img, 2.0, preserve_range=True)\n",
    "    width_img = gaussian(width_img, 1.0, preserve_range=True)\n",
    "    length_img = gaussian(length_img, 1.0, preserve_range=True)\n",
    "\n",
    "    return q_img, ang_img, width_img, length_img\n",
    "\n",
    "def cls_from_map(cls_map: torch.Tensor, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Expects input tensor cls_map to be of shape [batch_size, num_classes + 1, image_size, image_size]\n",
    "    cls_map[:, 0] is the confidence map\n",
    "    \"\"\"\n",
    "    cls_map = cls_map.permute(1, 0, 2, 3)\n",
    "    conf, preds = cls_map[0].round(), cls_map[1:]\n",
    "    conf = conf.repeat(preds.shape[0], 1, 1, 1)\n",
    "    masked_preds = conf * preds\n",
    "    masked_preds = masked_preds.view(masked_preds.shape[0], masked_preds.shape[1], -1)\n",
    "    masked_preds = masked_preds.mean(-1)\n",
    "    if verbose:\n",
    "        print(masked_preds)\n",
    "    masked_preds = masked_preds.argmax(0)\n",
    "    return masked_preds\n",
    "\n",
    "def grasp_rect_from_grasps(grasps: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expects inputs to be in the Grasp format with shape (N, 5).\n",
    "    Each of the N in the first dim represent a different grasp, with each being in the order\n",
    "    [center_x, center_y, angle, length, width]\n",
    "    \"\"\"\n",
    "    y, x, angle, length, width = grasps.transpose()\n",
    "    xo = np.cos(angle)\n",
    "    yo = np.sin(angle)\n",
    "\n",
    "    ya = y + length / 2 * yo\n",
    "    xa = x - length / 2 * xo\n",
    "    yb = y - length / 2 * yo\n",
    "    xb = x + length / 2 * xo\n",
    "\n",
    "    y1, x1 = ya - width / 2 * xo, xa - width / 2 * yo\n",
    "    y2, x2 = yb - width / 2 * xo, xb - width / 2 * yo\n",
    "    y3, x3 = yb + width / 2 * xo, xb + width / 2 * yo\n",
    "    y4, x4 = ya + width / 2 * xo, xa + width / 2 * yo\n",
    "\n",
    "    p1 = np.stack([x1, y1], 1)\n",
    "    p2 = np.stack([x2, y2], 1)\n",
    "    p3 = np.stack([x3, y3], 1)\n",
    "    p4 = np.stack([x4, y4], 1)\n",
    "\n",
    "    output_arr = np.stack([p1, p2, p3, p4], 1)\n",
    "    return output_arr\n",
    "\n",
    "def check_grasp_success(\n",
    "    predicted_rects: np.ndarray,\n",
    "    target_rects: np.ndarray,\n",
    "    image_size: int,\n",
    "    angle_threshold: float,\n",
    "    iou_threshold: float,\n",
    "    verbose: bool = False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Given an array list of predicted grasp rectangles and target grasp rectangles, outputs\n",
    "    if max_iou between at least one predicted grasp rectangle > threshold.\n",
    "    \"\"\"\n",
    "    from metrics.iou import max_iou # placed here due to circular import problems\n",
    "\n",
    "    for rect in predicted_rects:\n",
    "        iou_score = max_iou(\n",
    "            predicted_rect=rect, \n",
    "            target_rects=target_rects,\n",
    "            angle_threshold=angle_threshold,\n",
    "            image_size=image_size,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        if iou_score > iou_threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def grasps_from_map(\n",
    "    conf_map: np.ndarray,\n",
    "    angle_map: np.ndarray,\n",
    "    width_map: np.ndarray,\n",
    "    length_map: np.ndarray,\n",
    "    num_peaks: int,\n",
    "    verbose: bool = False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Computes and returns the top num_peaks grasps from the maps.    \n",
    "    A Grasp describes a bounding box in the following format\n",
    "    [center_x, center_y, angle, length, width]\n",
    "\n",
    "    Output is of shape (num_peaks, 5)\n",
    "    \"\"\"\n",
    "    local_max = peak_local_max(conf_map, min_distance=10, threshold_abs=0.2, num_peaks=num_peaks)\n",
    "\n",
    "    predicted_grasps = []\n",
    "    for grasp_coord in local_max:\n",
    "        grasp_coord = tuple(grasp_coord)\n",
    "        grasp_angle = angle_map[grasp_coord]\n",
    "        grasp_width = width_map[grasp_coord]\n",
    "        grasp_length = length_map[grasp_coord]\n",
    "        predicted_grasps.append((grasp_coord[0], grasp_coord[1], grasp_angle, grasp_width, grasp_length))\n",
    "\n",
    "    predicted_grasps = np.array(predicted_grasps)\n",
    "\n",
    "    if verbose:\n",
    "        print(predicted_grasps)\n",
    "\n",
    "    return predicted_grasps\n",
    "\n",
    "def map_based_iou(\n",
    "    conf_map: torch.Tensor,\n",
    "    cos_map: torch.Tensor,\n",
    "    sin_map: torch.Tensor,\n",
    "    width_map: torch.Tensor,\n",
    "    length_map: torch.Tensor,\n",
    "    target_grasp_rects: torch.Tensor,\n",
    "    angle_threshold: float = np.pi / 6,\n",
    "    iou_threshold: float = 0.25,\n",
    "    num_peaks: int = 3,\n",
    "    verbose: bool = False\n",
    "    ) -> bool:\n",
    "    \"\"\" \n",
    "    Returns a boolean value prediction whether the predicted grasps as successful or not\n",
    "    \"\"\"\n",
    "    target_grasp_rects = target_grasp_rects.numpy()\n",
    "    image_size = conf_map.shape[-1]\n",
    "\n",
    "    conf, angle, width, length = post_process_map_output(conf_map, cos_map, sin_map, width_map, length_map)\n",
    "    predicted_grasps = grasps_from_map(conf, angle, width, length, num_peaks=num_peaks, verbose=verbose)\n",
    "    if predicted_grasps.size == 0:\n",
    "        return False\n",
    "    predicted_grasp_rects = grasp_rect_from_grasps(predicted_grasps)\n",
    "    return check_grasp_success(predicted_grasp_rects, target_grasp_rects, image_size, angle_threshold, iou_threshold, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cls utility functions\n",
    "def cls_from_map(cls_map: torch.Tensor, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Expects input tensor cls_map to be of shape [batch_size, num_classes + 1, image_size, image_size]\n",
    "    cls_map[:, 0] is the confidence map\n",
    "    \"\"\"\n",
    "    cls_map = cls_map.permute(1, 0, 2, 3)\n",
    "    conf, preds = cls_map[0].round(), cls_map[1:]\n",
    "    conf = conf.repeat(preds.shape[0], 1, 1, 1)\n",
    "    masked_preds = conf * preds\n",
    "    masked_preds = masked_preds.view(masked_preds.shape[0], masked_preds.shape[1], -1)\n",
    "    masked_preds = masked_preds.mean(-1)\n",
    "    if verbose:\n",
    "        print(masked_preds)\n",
    "    masked_preds = masked_preds.argmax(0)\n",
    "    return masked_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "class MapBasedTrainer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            training_mode: str,\n",
    "            model: nn.Module,\n",
    "            device: str,\n",
    "            loss_fn: Any,\n",
    "            dataset: MapBasedJacquardDataset,\n",
    "            optimizer: optim.Optimizer,\n",
    "            lr: float,\n",
    "            train_batch_size: int,\n",
    "            test_split_ratio: float,\n",
    "            checkpoint_dir: str,\n",
    "            log_dir: str,\n",
    "            scheduler: Callable[[float, int], float] = lambda lr, step: lr,\n",
    "            num_accumulate_batches: int = 1,\n",
    "            test_batch_size: int = 16,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initializer for MapBasedTrainer;\n",
    "        \n",
    "        Arguments:\n",
    "            - training_mode: expected to be either \"grasp\" or \"cls\"\n",
    "            - model: torch model to be trained\n",
    "            - loss_fn: a callable loss function class that outputs the loss given predicted and target maps\n",
    "            - dataset: an instance of MapBasedJacquardDataset (cached gives better performance)\n",
    "            - optimizer: an uninitialized optimizer (eg. optimizer = torch.optim.Adam)\n",
    "            - log_dir: directory for saving tensorboard logs\n",
    "            - scheduler: a functions that takes the current lr and step number/epoch and outputs the next lr\n",
    "            - num_accumulate_batches: if the batch size is small, we may want to accumulate gradients over multiple batches\n",
    "                before updating model weights. This argument controls the number of batches we accumulate gradients for.\n",
    "        \"\"\"\n",
    "        self.training_mode = training_mode\n",
    "        assert self.training_mode == \"grasp\" or self.training_mode == \"cls\", \"training_mode must be 'grasp' or 'cls'\"\n",
    "\n",
    "        self.device = torch.device(device)\n",
    "        self.model = model.to(self.device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer(self.model.parameters(), self.lr)\n",
    "        self.num_accumulate_batches = num_accumulate_batches\n",
    "        self.scheduler = scheduler\n",
    "        self.log_dir = log_dir\n",
    "        self.tb_writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.mkdir(checkpoint_dir)\n",
    "\n",
    "        self.train_batch_size, self.test_batch_size = train_batch_size, test_batch_size\n",
    "        self.train_dataset = dataset\n",
    "        self.test_split_ratio = test_split_ratio\n",
    "\n",
    "        self.test_dataset = self.train_dataset.extract_test_dataset(self.test_split_ratio)\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.train_batch_size, shuffle=True, collate_fn=self.rect_collate)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.test_batch_size, shuffle=True, collate_fn=self.rect_collate)\n",
    "        self.step_number = 1\n",
    "\n",
    "    def run(self, num_steps: int = 10):\n",
    "        \"\"\"\n",
    "        Run function that runs the appropriate train functions based on whether training_mode was set to grasp or cls.\n",
    "        \"\"\"\n",
    "        if self.training_mode == \"grasp\":\n",
    "            self.grasp_run(num_steps)\n",
    "        elif self.training_mode == \"cls\":\n",
    "            self.cls_run(num_steps)\n",
    "\n",
    "    ################## Functions for saving and loading checkpoints ##################\n",
    "    @staticmethod\n",
    "    def load_state(load_path: str) -> MapBasedTrainer:\n",
    "        \"\"\"\n",
    "        Returns a MapBasedTrainer object loaded from the checkpoint file located at load_path.\n",
    "        \"\"\"\n",
    "        f = open(load_path, \"rb\")\n",
    "        trainer_obj = pickle.load(f)\n",
    "        trainer_obj.tb_writer = SummaryWriter(log_dir=trainer_obj.log_dir)\n",
    "        return trainer_obj\n",
    "    \n",
    "    def save_state(self, grasp_or_cls: str, iteration: int, save_loss: float, save_acc: float, decimal_place: int = 6):\n",
    "        \"\"\"\n",
    "        Saves the entire MapBasedTrainer class as a serialized pickle object. This saves both model weights\n",
    "        and training state (iteration number, optimizer state, current lr, etc.).\n",
    "\n",
    "        File names are generated according to model type (grasp or cls) and test metrics of most recent test step.\n",
    "        \"\"\"\n",
    "        tb_writer = self.tb_writer\n",
    "        self.tb_writer = None\n",
    "        save_name = grasp_or_cls + \"_Step_\" + str(iteration) + \"_Acc_\" + str(round(save_acc, decimal_place)) + \"_Loss_\" + str(round(save_loss, decimal_place)) + \".pth\"\n",
    "        save_path = os.path.join(self.checkpoint_dir, save_name)\n",
    "        with open(save_path, \"wb\") as outfile:\n",
    "            pickle.dump(self, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "        self.tb_writer = tb_writer\n",
    "\n",
    "\n",
    "    ################## Grasping functions ##################\n",
    "    def grasp_train_step(self):\n",
    "        \"\"\"\n",
    "        Runs a single grasp train step (trains model on the entire dataset once)\n",
    "        \"\"\"\n",
    "        self.model = self.model.train()\n",
    "        loop = tqdm(self.train_loader, total=len(self.train_loader), leave=True, position=0)\n",
    "        loop.set_description(f\"Grasp training step {self.step_number}\")\n",
    "\n",
    "        for i, (rgbd_image, target_grasp_maps, _, _) in enumerate(loop):\n",
    "            rgbd_image, target_grasp_maps = rgbd_image.to(self.device), target_grasp_maps.to(self.device)\n",
    "            predicted_grasp_maps = self.model(rgbd_image)\n",
    "            loss = self.loss_fn(predicted_grasp_maps, target_grasp_maps)\n",
    "            loss.backward()\n",
    "            if i % self.num_accumulate_batches == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            loop.set_postfix(loss = loss.item())\n",
    "\n",
    "    def grasp_test_step(self):\n",
    "        \"\"\"\n",
    "        Runs a single grasp test step (computes test metrics for model on entire test dataset)\n",
    "        \"\"\"\n",
    "        self.model = self.model.eval()\n",
    "        loop = tqdm(self.test_loader, total=len(self.test_loader), leave=True, position=0)\n",
    "        loop.set_description(f\"Grasp test step {self.step_number}\")\n",
    "\n",
    "        total_loss = 0\n",
    "        num_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for rgbd_image, target_grasp_maps, target_grasp_rects, _ in loop:\n",
    "                rgbd_image, target_grasp_maps = rgbd_image.to(self.device), target_grasp_maps.to(self.device)\n",
    "                predicted_maps = self.model(rgbd_image)\n",
    "                loss = self.loss_fn(predicted_maps, target_grasp_maps)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                for grasp_map, target_rect in zip(predicted_maps, target_grasp_rects):\n",
    "                    conf, cos, sin, width, length = grasp_map\n",
    "                    num_correct += map_based_iou(conf, cos, sin, width, length, target_rect)\n",
    "\n",
    "        avg_loss = total_loss / len(self.test_loader)\n",
    "        avg_acc = num_correct / len(self.test_dataset)\n",
    "        print(f\"Average Loss: {avg_loss} | Accuracy: {avg_acc}\")\n",
    "        return avg_loss, avg_acc\n",
    "\n",
    "    def grasp_run(self, num_steps: int = 10):\n",
    "        \"\"\"\n",
    "        This function does the following;\n",
    "            - Runs a grasp train step\n",
    "            - Runs a grasp test step\n",
    "            - Saves model weights and training state\n",
    "            - Updates lr based on self.scheduler\n",
    "        \"\"\"\n",
    "        while self.step_number <= num_steps:\n",
    "            print(\"-\" * 50)\n",
    "            self.grasp_train_step()\n",
    "            test_loss, test_acc = self.grasp_test_step()\n",
    "\n",
    "            self.tb_writer.add_scalar(\"Grasp Test loss\", test_loss, self.step_number)\n",
    "            self.tb_writer.add_scalar(\"Grasp Test Accuracy\", test_acc, self.step_number)\n",
    "\n",
    "            self.save_state(\"Grasp\", self.step_number, test_loss, test_acc)\n",
    "            self.step_number += 1\n",
    "            self.set_lr(self.scheduler(self.lr, self.step_number))\n",
    "    \n",
    "\n",
    "    ################## Classification functions ##################\n",
    "    def cls_train_step(self):\n",
    "        \"\"\"\n",
    "        Runs a single cls train step (trains model on the entire dataset once)\n",
    "        \"\"\"\n",
    "        self.model = self.model.train()\n",
    "        loop = tqdm(self.train_loader, total=len(self.train_loader), leave=True, position=0)\n",
    "        loop.set_description(f\"Cls training step {self.step_number}\")\n",
    "\n",
    "        for i, (rgbd_image, _, _, cls_maps) in enumerate(loop):\n",
    "            rgbd_image, cls_maps = rgbd_image.to(self.device), cls_maps.to(self.device)\n",
    "            predicted_cls_maps = self.model(rgbd_image)\n",
    "            loss = self.loss_fn(predicted_cls_maps, cls_maps)\n",
    "            loss.backward()\n",
    "            if i % self.num_accumulate_batches == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            loop.set_postfix(loss = loss.item())\n",
    "\n",
    "    def cls_test_step(self):\n",
    "        \"\"\"\n",
    "        Runs a single cls test step (computes test metrics on entire test dataset)\n",
    "        \"\"\"\n",
    "        self.model = self.model.eval()\n",
    "        loop = tqdm(self.test_loader, total=len(self.test_loader), leave=True, position=0)\n",
    "        loop.set_description(f\"Cls test step {self.step_number}\")\n",
    "\n",
    "        total_loss = 0\n",
    "        num_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for rgbd_image, _, _, cls_maps in loop:\n",
    "                rgbd_image, cls_maps = rgbd_image.to(self.device), cls_maps.to(self.device)\n",
    "                predicted_cls_maps = self.model(rgbd_image)\n",
    "                loss = self.loss_fn(predicted_cls_maps, cls_maps)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                predicted_labels = cls_from_map(predicted_cls_maps)\n",
    "                target_labels = cls_from_map(cls_maps)\n",
    "                num_correct += (predicted_labels == target_labels).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.test_loader)\n",
    "        avg_acc = num_correct / len(self.test_dataset)\n",
    "        print(f\"Average Loss: {avg_loss} | Accuracy: {avg_acc}\")\n",
    "        return avg_loss, avg_acc\n",
    "    \n",
    "    def cls_run(self, num_steps: int = 10):\n",
    "        \"\"\"\n",
    "        This function does the following;\n",
    "            - Runs a cls train step\n",
    "            - Runs a cls test step\n",
    "            - Saves model weights and training state\n",
    "            - Updates lr based on self.scheduler\n",
    "        \"\"\"\n",
    "        while self.step_number <= num_steps:\n",
    "            print(\"-\" * 50)\n",
    "            self.cls_train_step()\n",
    "            test_loss, test_acc = self.cls_test_step()\n",
    "\n",
    "            self.tb_writer.add_scalar(\"Cls Test loss\", test_loss, self.step_number)\n",
    "            self.tb_writer.add_scalar(\"Cls Test Accuracy\", test_acc, self.step_number)\n",
    "\n",
    "            self.save_state(\"Cls\", self.step_number, test_loss, test_acc)\n",
    "            self.step_number += 1\n",
    "            self.set_lr(self.scheduler(self.lr, self.step_number))\n",
    "\n",
    "\n",
    "    ################## Utility functions ##################\n",
    "    def set_lr(self, lr: float):\n",
    "        self.lr = lr\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p[\"lr\"] = lr\n",
    "\n",
    "    def rect_collate(self, batch):\n",
    "        rgbds = []\n",
    "        grasp_maps = []\n",
    "        rects = []\n",
    "        cls_maps = []\n",
    "        for a, b, c, d in batch:\n",
    "            rgbds.append(a.unsqueeze(0))\n",
    "            grasp_maps.append(b.unsqueeze(0))\n",
    "            rects.append(c)\n",
    "            cls_maps.append(d.unsqueeze(0))\n",
    "        return torch.cat(rgbds, dim=0), torch.cat(grasp_maps, dim=0), rects, torch.cat(cls_maps, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MapBasedJacquardDataset(\n",
    "    image_size = 224, \n",
    "    precision = torch.float32,\n",
    "    cache_path = \"/Users/gursi/Desktop/jacquard/cache\",\n",
    "    random_augment = True\n",
    ")\n",
    "\n",
    "model = GRConvNet4(clip=True)\n",
    "loss_fn = CrossEntropyMapLoss()\n",
    "lr = 1e-6\n",
    "optimizer = torch.optim.Adam\n",
    "\n",
    "# Scheduler that halves learning rate every 25 iterations\n",
    "def scheduler(lr, step):\n",
    "    if (step+1) % 25 == 0:\n",
    "        return lr/2\n",
    "    return lr\n",
    "\n",
    "trainer = MapBasedTrainer(\n",
    "    training_mode = \"cls\",\n",
    "    model = model,\n",
    "    device = \"mps\",\n",
    "    loss_fn = loss_fn,\n",
    "    dataset = dataset,\n",
    "    optimizer = optimizer,\n",
    "    lr = lr,\n",
    "    train_batch_size = 8,\n",
    "    test_split_ratio = 0.2,\n",
    "    checkpoint_dir = \"/Users/gursi/Desktop/new_trials\",\n",
    "    log_dir = \"logs\",\n",
    "    scheduler = scheduler,\n",
    "    num_accumulate_batches = 8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cls training step 1: 100%|| 111/111 [00:25<00:00,  4.42it/s, loss=0.619]\n",
      "Cls test step 1: 100%|| 14/14 [00:05<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6246466892106193 | Accuracy: 0.3963963963963964\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cls training step 2: 100%|| 111/111 [00:21<00:00,  5.13it/s, loss=0.594]\n",
      "Cls test step 2: 100%|| 14/14 [00:02<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6052048121179853 | Accuracy: 0.3963963963963964\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cls training step 3: 100%|| 111/111 [00:21<00:00,  5.14it/s, loss=0.665]\n",
      "Cls test step 3: 100%|| 14/14 [00:02<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.587119996547699 | Accuracy: 0.3963963963963964\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cls training step 4:  47%|     | 52/111 [00:10<00:11,  5.15it/s, loss=0.575]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 67\u001b[0m, in \u001b[0;36mMapBasedTrainer.run\u001b[0;34m(self, num_steps)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrasp_run(num_steps)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 217\u001b[0m, in \u001b[0;36mMapBasedTrainer.cls_run\u001b[0;34m(self, num_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_number \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m num_steps:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_test_step()\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb_writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCls Test loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_number)\n",
      "Cell \u001b[0;32mIn[10], line 171\u001b[0m, in \u001b[0;36mMapBasedTrainer.cls_train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    169\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCls training step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgbd_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_maps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrgbd_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_maps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrgbd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_maps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredicted_cls_maps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgbd_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.11/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 244\u001b[0m, in \u001b[0;36mMapBasedTrainer.rect_collate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    242\u001b[0m     rects\u001b[38;5;241m.\u001b[39mappend(c)\n\u001b[1;32m    243\u001b[0m     cls_maps\u001b[38;5;241m.\u001b[39mappend(d\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(rgbds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), torch\u001b[38;5;241m.\u001b[39mcat(grasp_maps, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), rects, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
